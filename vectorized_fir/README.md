# Vectorial Scalar Add Example

This example code includes the function _task_VectScalarAdd_ that computes the sum *C = a + B*, where *B* and *C* are integer arrays of N elements and *a* is an integer constant.


## Measuring and Collecting Performance Counters
Every RISC-V core of the PULP Platform includes several performance counters to measure the performance statistics of the taks execution. 
The available performance counters can be found [here](https://github.com/pulp-platform/pulp-sdk/blob/main/rtos/pmsis/pmsis_api/include/pmsis/chips/default.h). 
The [PMISIS APIs](https://greenwaves-technologies.com/manuals/BUILD/PMSIS_API/html/group__Perf.html) are used to configure, control and read the available counters.

```
pi_perf_conf( 1 << PI_PERF_CYCLES | 1 << PI_PERF_INSTR ); // enable the perf counters of interest

pi_perf_reset(); // reset the performance counters
pi_perf_start(); // start the performance counters

foo(); // task to profile

pi_perf_stop(); // stop the performance counters

// collect an print statistics
uint32_t instr_cnt = pi_perf_read(PI_PERF_INSTR);
uint32_t cycles_cnt = pi_perf_read(PI_PERF_CYCLES);
printf("Number of Instructions: %d\nClock Cycles: %d\n", 
        instr_cnt, cycles_cnt);
```

## Getting Started
To get started with the example, simply:
~~~~~shell
cd vector_scalar_add/
make clean all run
~~~~~
* How many instructions and clock cycles are taken by the _task_VectScalarAdd_ function?
* What is it the measured CPI?
* How many of the instruction corresponds to load and stall and how many stalls are included with the count of the clock cycles?

Try to relate the number of instructions and clock cycles with the assembly code, which can be obtained by:
~~~~~shell
make dis > test.s
~~~~~

## System Traces
Optionally, to better undestand the what it is going on at runtime, the developers may want to dump the [System Traces](https://gvsoc.readthedocs.io/en/latest/system_traces.html).
To dump them simply:
~~~~~shell
make clean all run runner_args="--trace=<PATH>"
~~~~~
the \<PATH\> should be replaced with the name of the events of interest, which are generated by the different modules (FC core, cluster cores, cache, uDMA, etc..). 
To dump all the possibile events and store the into a _log.txt_ file within BUILD/PULP/GCC_RISCV/ folder, you can use: 
~~~~~shell
make clean all run runner_args="--trace=.*:log.txt"
~~~~~
Alternatively, to only print the events generated by the FC core:
~~~~~shell
make clean all run runner_args="--trace=/sys/board/chip/soc/fc/insn:log.txt"
~~~~~
To read the trace output, you can refer to the following syntax:
```
4890000: 489: [/sys/board/chip/soc/cluster/pe0/insn] M 1c001252 p.sw  0, 4(a5!)  a5=10000010  a5:1000000c  PA:1000000c
<timestamp> <cycles> <path> <address> <instruction> <operands> <operands info>
```
where:
* \<timestamp\> is the timestamp of the event in picoseconds
* \<cycles\> is the number of cycles
* \<path\> is the path in the architecture where the event occurred
* \<address\> is the address of the instruction
* \<instruction\> is the instruction label
* \<operands\> is the part of the decoded operands
* \<operands info\> is giving details about the operands values and how they are used

The timestamp is absolute. The cycle count is local to the frequency domain.

## Optimizing the code 
Comment out in the Makefile the following flag:
```
# APP_CFLAGS += -mnohwloop
```
What happened to the system performance? Can we further improve the CPI?


